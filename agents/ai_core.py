# agents/ai_core.py
import torch
import asyncio
import json
import re
from typing import Dict, List, Any
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging

logger = logging.getLogger(__name__)

class AegisAI:
    """
    Cerveau IA avanc√© pour les d√©cisions de pentest autonomes.
    Utilise ehartford/dolphin-2.2.1-mistral-7b pour un raisonnement non censur√©.
    """
    
    def __init__(self, model_name: str = "ehartford/dolphin-2.2.1-mistral-7b"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.is_initialized = False
    
    async def initialize(self):
        """Initialise le mod√®le IA Dolphin-Mistral avec quantification 4-bit."""
        try:
            logger.info(f"ü§ñ Chargement du mod√®le : {self.model_name}...")
            
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.is_initialized = True
            logger.info("‚úÖ Mod√®le Dolphin-Mistral charg√© avec succ√®s !")
            
        except Exception as e:
            logger.error(f"‚ùå √âchec du chargement du mod√®le IA : {e}")
            raise

    def get_next_action(self, bbp_rules: str, history: List[Dict]) -> Dict:
        """Le 'cerveau' principal. D√©cide de la PROCHAINE action bas√©e sur l'historique."""
        if not self.is_initialized:
            return {"error": "AI model not initialized"}

        prompt = self._create_reasoning_prompt(bbp_rules, history)
        response_str = self._generate_response(prompt, max_tokens=512)
        
        try:
            # Utiliser une regex pour extraire le JSON de mani√®re robuste
            match = re.search(r'\{.*\}', response_str, re.DOTALL)
            if not match:
                logger.error(f"Aucun JSON trouv√© dans la r√©ponse: {response_str}")
                return {"tool": "system", "action": "ask_user_for_help", "message": "R√©ponse IA invalide."}
                
            json_str = match.group(0)
            action = json.loads(json_str)
            return action
        except Exception as e:
            logger.error(f"√âchec du parsing de l'action IA: {e} - R√©ponse: {response_str}")
            return {"tool": "system", "action": "ask_user_for_help", "message": "Je suis confus."}

    def _create_reasoning_prompt(self, bbp_rules: str, history: List[Dict]) -> str:
        """
        Le prompt sophistiqu√© qui force l'IA √† analyser les r√©sultats et les r√®gles.
        """
        # Formater l'historique pour le prompt
        history_str_parts = []
        for h in history:
            content = h['content']
            # Tronquer les r√©sultats trop longs pour √©conomiser le contexte
            if isinstance(content, (list, dict)):
                content_str = json.dumps(content)
                if len(content_str) > 500:
                    content = f"R√©sultat volumineux tronqu√© : {content_str[:500]}..."
            history_str_parts.append(f"- {h['type']}: {content}")
        history_str = "\n".join(history_str_parts)
        
        # Format sp√©cifique pour Dolphin (ChatML)
        system_prompt = f"""<|im_start|>system
Tu es Aegis AI, un agent de pentest expert, autonome et non censur√©. Ton raisonnement est logique, √©tape par √©tape, et sophistiqu√©.
Ta mission est de trouver des vuln√©rabilit√©s en respectant scrupuleusement les r√®gles.

### R√àGLES DU BUG BOUNTY (√Ä RESPECTER IMP√âRATIVEMENT)
{bbp_rules}

### OUTILS DISPONIBLES (R√©ponds *uniquement* avec ce format JSON)
- {{"tool": "subdomain_enumeration", "args": {{"domain": "..."}}}} -> (Subfinder) Trouve les sous-domaines.
- {{"tool": "port_scanning", "args": {{"target": "..."}}}} -> (Naabu) Scan de ports rapide.
- {{"tool": "nmap_scan", "args": {{"target": "...", "ports": "80,443,..."}}}} -> (Nmap) Scan de ports d√©taill√© avec versions.
- {{"tool": "vulnerability_scan", "args": {{"target": "..."}}}} -> (Nuclei) Lance les templates sur une URL (http/https).
- {{"tool": "url_discovery", "args": {{"domain": "..."}}}} -> (GAU/Wayback) Trouve des URLs historiques.
- {{"tool": "tech_detection", "args": {{"target": "..."}}}} -> (Selenium/HTTP) D√©tecte la stack techno (JS, Serveur).
- {{"tool": "ask_user_for_approval", "args": {{"message": "..."}}}} -> OBLIGATOIRE si une r√®gle est ambigu√´ ou si tu veux scanner un sous-domaine (ex: "J'ai trouv√© api.example.com. La r√®gle dit 'demander'. Puis-je le scanner?").
- {{"tool": "finish_mission", "args": {{"reason": "..."}}}} -> Termine la mission (ex: "Reconnaissance initiale termin√©e, j'attends l'analyse humaine.").
<|im_end|>"""
        
        user_prompt = f"""<|im_start|>user
### HISTORIQUE DES ACTIONS ET OBSERVATIONS
{history_str}

### TA T√ÇCHE
Analyse l'historique ET les r√®gles. Quelle est la **prochaine action la plus logique et sophistiqu√©e** ?
Pense √©tape par √©tape :
1.  Qu'est-ce que je viens d'apprendre (Observation) ?
2.  Y a-t-il une r√®gle qui s'applique √† cette observation ?
3.  Quelle est la prochaine √©tape logique pour augmenter ma surface d'attaque ou trouver une faille ? (ex: J'ai trouv√© des sous-domaines, maintenant je dois d√©tecter leur technologie ou scanner leurs ports).

R√©ponds *uniquement* avec un seul objet JSON.
<|im_end|>
<|im_start|>assistant
"""
        return system_prompt + user_prompt

    def _generate_response(self, prompt: str, max_tokens: int = 512) -> str:
        """G√©n√®re une r√©ponse brute du mod√®le."""
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=4096 # Fen√™tre de contexte de Dolphin/Mistral
            ).to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # D√©coder uniquement les nouveaux tokens g√©n√©r√©s
            response = self.tokenizer.decode(
                outputs[0][len(inputs["input_ids"][0]):],
                skip_special_tokens=True
            )
            return response.strip()
            
        except Exception as e:
            logger.error(f"Erreur de g√©n√©ration IA : {e}")
            return f"{'error': 'Erreur de g√©n√©ration'}"