# agents/ai_core.py
# --- VERSION MODIFI√âE (v4.0 - Conversationnel) ---

import torch
import asyncio
import json
import re
from typing import Dict, List, Any
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging
from agents.learning_engine import AegisLearningEngine

logger = logging.getLogger(__name__)

class AegisAI:
    """
    Cerveau IA avanc√© v4.0
    Capacit√©s : Triage Conversationnel (Niveau 1)
               Ex√©cution Autonome (Niveau 2)
               Auto-Apprentissage
    """
    
    def __init__(self, learning_engine: AegisLearningEngine, model_name: str = "ehartford/dolphin-2.2.1-mistral-7b"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.is_initialized = False
        self.learning_engine = learning_engine
        self.learned_patterns = "" # La m√©moire des le√ßons apprises
    
    async def initialize(self):
        """Initialise le mod√®le IA ET charge les le√ßons apprises."""
        try:
            logger.info(f"ü§ñ Chargement du mod√®le : {self.model_name}...")
            
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.is_initialized = True
            logger.info("‚úÖ Mod√®le Dolphin-Mistral charg√© avec succ√®s !")

            # --- CHARGEMENT DE LA M√âMOIRE (APPRENTISSAGE) ---
            logger.info("üß† Chargement des le√ßons des missions pr√©c√©dentes...")
            loop = asyncio.get_event_loop()
            self.learned_patterns = await loop.run_in_executor(None, self.learning_engine.load_learned_patterns)
            logger.info("‚úÖ Le√ßons charg√©es dans la m√©moire de l'IA.")
            
        except Exception as e:
            logger.error(f"‚ùå √âchec du chargement du mod√®le IA : {e}", exc_info=True)
            raise

    # --- NIVEAU 1 : AGENT DE TRIAGE (Conversationnel) ---
    # NOUVELLE FONCTION
    async def triage_mission(self, conversation_history: List[Dict]) -> Dict:
        """
        Analyse la conversation naturelle et d√©termine si la mission est pr√™te.
        R√©pond par une question (langage naturel) ou un signal de d√©marrage (JSON).
        """
        if not self.is_initialized:
            return {"response_type": "error", "text": "IA non initialis√©e."}

        prompt = self._create_triage_prompt(conversation_history)
        # Utiliser moins de tokens pour une r√©ponse de triage rapide
        response_str = self._generate_response(prompt, max_tokens=256)
        
        try:
            # Tenter d'abord de trouver un JSON (signal de d√©marrage)
            match = re.search(r'\{.*\}', response_str, re.DOTALL)
            if match:
                json_str = match.group(0)
                action = json.loads(json_str)
                # S'assurer que c'est bien le bon signal
                if action.get("response_type") == "start_mission":
                    logger.info("Agent de Triage a valid√© la mission, passage au Niveau 2.")
                    return action
            
            # Si ce n'est pas un JSON de d√©marrage, c'est une question
            # Nettoyer la r√©ponse au cas o√π le mod√®le ajouterait <|im_end|>
            response_text = response_str.split('<|im_end|>')[0].strip()
            return {"response_type": "question", "text": response_text}
            
        except Exception as e:
            logger.error(f"√âchec du parsing de l'action de triage: {e} - R√©ponse: {response_str}")
            return {"response_type": "question", "text": "Je suis confus. Pouvez-vous reformuler ?"}

    # NOUVELLE FONCTION
    def _create_triage_prompt(self, conversation_history: List[Dict]) -> str:
        """
        Prompt pour l'agent de Triage (Niveau 1).
        Objectif : Obtenir Cible et R√®gles.
        """
        history_str = "\n".join([f"{h['role']}: {h['content']}" for h in conversation_history])
        
        system_prompt = """<|im_start|>system
Tu es Aegis AI, un planificateur de mission de cybers√©curit√©. Ton but est de dialoguer avec l'utilisateur pour collecter TOUTES les informations n√©cessaires avant de lancer une mission.

Les informations requises sont :
1.  **LA CIBLE** (ex: "example.com", "192.168.1.1", ou un fichier comme "image.png").
2.  **LES R√àGLES** (ex: "scope", "out-of-scope", "ne pas faire de DDoS", "c'est un CTF").

TA T√ÇCHE :
- Analyse la conversation.
- Si une information manque (cible ou r√®gles), pose une question CLAIRE et COURTE √† l'utilisateur.
- Si l'utilisateur te donne une information, accuse r√©ception et demande la suite (ex: "OK, cible 'example.com' re√ßue. Quelles sont les r√®gles du scope ?").
- **NE LANCE PAS DE SCAN TOI-M√äME.**

- **UNE FOIS QUE TU AS TOUTES LES INFORMATIONS (CIBLE + R√àGLES)**, ne pose plus de question. R√©ponds *uniquement* avec l'objet JSON suivant pour d√©marrer la mission :
```json
{
  "response_type": "start_mission",
  "target": "[la cible principale]",
  "rules": "[r√©sum√© de toutes les r√®gles et instructions]"
}
````

\<|im\_end|\>"""

```
    user_prompt = f"""<|im_start|>user
```

Voici notre conversation jusqu'√† pr√©sent :
{history\_str}

Quelle est ta prochaine r√©ponse ? (Soit une question pour obtenir plus d'infos, soit le JSON de d√©marrage si tu as TOUT)
\<|im\_end|\>
\<|im\_start|\>assistant
"""
return system\_prompt + user\_prompt

```
# --- NIVEAU 2 : AGENT EX√âCUTEUR (Autonome) ---
# (Fonction existante - pas de changement)
def get_next_action(self, bbp_rules: str, history: List[Dict]) -> Dict:
    """Le 'cerveau' Ex√©cuteur (v3.0). Ne parle qu'en JSON."""
    if not self.is_initialized:
        return {"error": "AI model not initialized"}

    prompt = self._create_reasoning_prompt(bbp_rules, history, self.learned_patterns)
    response_str = self._generate_response(prompt, max_tokens=1024)
    
    try:
        match = re.search(r'\{.*\}', response_str, re.DOTALL)
        if not match:
            logger.error(f"Aucun JSON trouv√© dans la r√©ponse (Niveau 2): {response_str}")
            return {"tool": "system", "action": "ask_user_for_help", "message": "R√©ponse IA invalide."}
            
        json_str = match.group(0)
        action = json.loads(json_str)
        return action
    except Exception as e:
        logger.error(f"√âchec du parsing de l'action IA (Niveau 2): {e} - R√©ponse: {response_str}")
        return {"tool": "system", "action": "ask_user_for_help", "message": "Je suis confus."}

# (Fonction existante - pas de changement)
def _create_reasoning_prompt(self, bbp_rules: str, history: List[Dict], learned_patterns: str) -> str:
    
    Le prompt Ex√©cuteur (v3.0). Lit les le√ßons et les r√®gles, sort du JSON.
        
    history_str_parts = []
    for h in history:
        content = h['content']
        if isinstance(content, (list, dict)):
            content_str = json.dumps(content)
            if len(content_str) > 500:
                content = f"R√©sultat volumineux tronqu√© : {content_str[:500]}..."
        history_str_parts.append(f"- {h['type']}: {content}")
    history_str = "\n".join(history_str_parts)
    
    system_prompt = f"""<|im_start|>system

Tu es Aegis AI, un agent de pentest expert (Mode Ex√©cuteur). Tu es en mission.
Tu ne parles *pas* √† l'utilisateur. Tu ne g√©n√®res *que* des actions JSON.
Tu dois analyser l'historique de la mission, les r√®gles, et tes le√ßons apprises pour d√©cider de la prochaine action outil.

### LE√áONS DES MISSIONS PR√âC√âDENTES (Ta M√©moire)

{learned\_patterns}

### R√àGLES DU BUG BOUNTY (√Ä RESPECTER IMP√âRATIVEMENT)

{bbp\_rules}

### OUTILS DISPONIBLES (R√©ponds *uniquement* avec ce format JSON)

#### Phase 1: Reconnaissance

  - {{"tool": "subdomain\_enumeration", "args": {{"domain": "..."}}}}
  - {{"tool": "port\_scanning", "args": {{"target": "..."}}}}
  - {{"tool": "nmap\_scan", "args": {{"target": "...", "ports": "80,443,..."}}}}
  - {{"tool": "url\_discovery", "args": {{"domain": "..."}}}}
  - {{"tool": "tech\_detection", "args": {{"target": "..."}}}}

#### Phase 2: Analyse de Vuln√©rabilit√©s

  - {{"tool": "vulnerability\_scan", "args": {{"target": "..."}}}} -\> (Nuclei)
  - {{"tool": "run\_sqlmap", "args": {{"target": "..."}}}} -\> (Sqlmap)
  - {{"tool": "fetch\_url", "args": {{"target": "..."}}}} -\> (Test IDOR)
  - {{"tool": "discover\_interactables", "args": {{"target": "..."}}}} -\> (Selenium "Regarder")
  - {{"tool": "test\_form\_payload", "args": {{"target": "...", "form\_identifier": "...", "input\_payloads": {{...}}}}}} -\> (Selenium "Tester")

#### Phase 3: Syst√®me

  - {{"tool": "ask\_user\_for\_approval", "args": {{"message": "..."}}}} -\> OBLIGATOIRE si une r√®gle est ambigu√´.

  - {{"tool": "finish\_mission", "args": {{"reason": "..."}}}}
    \<|im\_end|\>"""

      user_prompt = f"""<|im_start|>user

### HISTORIQUE DES ACTIONS ET OBSERVATIONS (Mission Actuelle)

{history\_str}

### TA T√ÇCHE

Analyse l'historique, les r√®gles, ET tes le√ßons apprises. Quelle est la prochaine action JSON ?
Pense √©tape par √©tape :

1.  **Observation** : Qu'est-ce que je viens d'apprendre ?
2.  **M√©moire (Le√ßons)** : Est-ce un faux positif connu ?
3.  **R√®gles** : Est-ce que j'enfreins une r√®gle ?
4.  **Hypoth√®se** : Quelle est l'action JSON la plus logique ?

R√©ponds *uniquement* avec un seul objet JSON.
\<|im\_end|\>
\<|im\_start|\>assistant
"""
return system\_prompt + user\_prompt

# (Fonction existante - pas de changement)
def _generate_response(self, prompt: str, max_tokens: int = 1024) -> str:
    """G√©n√®re une r√©ponse brute du mod√®le."""
    try:
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = self.tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]):],
            skip_special_tokens=True
        )
        return response.strip()
        
    except Exception as e:
        logger.error(f"Erreur de g√©n√©ration IA : {e}")
        return f"{'error': 'Erreur de g√©n√©ration'}"
    
