# OpenRouter API Configuration
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# ==============================================================================
# API KEY SHARDING (Optional - Advanced Cost Control)
# ==============================================================================
# These allow you to use different API keys for different model roles to:
# - Track costs per model type
# - Prevent a single model from consuming the entire quota
# - Implement granular rate limiting and budget control
#
# If left blank, all models will fallback to OPENROUTER_API_KEY
# ==============================================================================

# Strategic Planning & Management API Key (for Orchestrator/Manager role)
# Used by: Strategic Planner & Triage Agent
STRATEGIC_API_KEY=

# Reasoning & Vulnerability Analysis API Key (for DeepSeek/Thinking role)
# Used by: Reasoning & Vulnerability Analyst
REASONING_API_KEY=

# Code Analysis & Generation API Key (for Qwen/Coder role)
# Used by: Code Analyst & Payload Engineer
CODE_API_KEY=

# Visual & Multimodal Analysis API Key (for Vision tasks)
# Used by: Visual Analyst & UI Reconnaissance
VISUAL_API_KEY=

# Optional: Proxy configuration for stealth mode (comma-separated list)
# PROXY_LIST=http://proxy1:port,http://proxy2:port

# ==============================================================================
# LLM MODEL CONFIGURATION
# ==============================================================================
# You can easily change any of these models to use different LLMs from OpenRouter
# Just paste the model identifier and your API key - no need to touch Python code!
# Available models: https://openrouter.ai/models
# ==============================================================================

# Strategic Planning & Triage Model
# Role: Handles high-level decision making, mission planning, triage, scope analysis, and risk assessment
# Default: nousresearch/hermes-3-llama-3.1-70b
STRATEGIC_MODEL=nousresearch/hermes-3-llama-3.1-70b

# Reasoning & Vulnerability Analysis Model
# Role: Specialized in vulnerability analysis, reasoning about exploits, and security assessment
# Default: cognitivecomputations/dolphin3.0-r1-mistral-24b
REASONING_MODEL=cognitivecomputations/dolphin3.0-r1-mistral-24b

# Code Analysis & Payload Engineering Model
# Role: Focuses on analyzing code, generating exploit payloads, writing scripts, and technical implementation
# Default: qwen/qwen-2.5-72b-instruct
CODE_MODEL=qwen/qwen-2.5-72b-instruct

# Visual Analysis & UI Reconnaissance Model (Multimodal)
# Role: Analyzes screenshots to understand UI layouts, identify clickable elements, and detect visual vulnerabilities
# Default: qwen/qwen2.5-vl-32b-instruct:free
VISUAL_MODEL=qwen/qwen2.5-vl-32b-instruct:free

# ==============================================================================
# LLM GENERATION PARAMETERS
# ==============================================================================
# These control the behavior of the LLM responses
# Temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
# Max Tokens: Maximum length of generated responses
# ==============================================================================

# Default temperature for all models (can be overridden per-call)
# Range: 0.0 to 1.0
# Lower values (0.3-0.5) = more focused and deterministic
# Higher values (0.7-0.9) = more creative and varied
DEFAULT_TEMPERATURE=0.7

# Default maximum tokens to generate
# Controls the maximum length of responses from the LLM
# Higher values allow longer, more detailed responses but cost more
DEFAULT_MAX_TOKENS=4096

# ==============================================================================
# LEGACY COMPATIBILITY (DEPRECATED - Use above variables instead)
# ==============================================================================
# These are kept for backward compatibility but will be removed in future versions
# ORCHESTRATOR_MODEL, CODER_MODEL are mapped to STRATEGIC_MODEL and CODE_MODEL
